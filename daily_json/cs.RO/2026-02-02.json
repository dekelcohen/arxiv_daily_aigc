[
    {
        "title": "OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth",
        "summary": "Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.",
        "url": "http://arxiv.org/abs/2602.01268v1",
        "published_date": "2026-02-01T14:57:33+00:00",
        "updated_date": "2026-02-01T14:57:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jaehyeon Cho",
            "Jhonghyun An"
        ]
    },
    {
        "title": "Reinforcement Learning for Active Perception in Autonomous Navigation",
        "summary": "This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.",
        "url": "http://arxiv.org/abs/2602.01266v1",
        "published_date": "2026-02-01T14:54:40+00:00",
        "updated_date": "2026-02-01T14:54:40+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Grzegorz Malczyk",
            "Mihir Kulkarni",
            "Kostas Alexis"
        ]
    },
    {
        "title": "SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models",
        "summary": "Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., \"Form a circle\") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.",
        "url": "http://arxiv.org/abs/2602.01226v1",
        "published_date": "2026-02-01T13:34:34+00:00",
        "updated_date": "2026-02-01T13:34:34+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Aditya Shibu",
            "Marah Saleh",
            "Mohamed Al-Musleh",
            "Nidhal Abdulaziz"
        ]
    },
    {
        "title": "SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment",
        "summary": "We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.",
        "url": "http://arxiv.org/abs/2602.01189v1",
        "published_date": "2026-02-01T12:24:12+00:00",
        "updated_date": "2026-02-01T12:24:12+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Astik Srivastava",
            "Thomas J Chackenkulam. Bitla Bhanu Teja",
            "Antony Thomas",
            "Madhava Krishna"
        ]
    },
    {
        "title": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \\href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.",
        "url": "http://arxiv.org/abs/2602.01166v1",
        "published_date": "2026-02-01T11:34:37+00:00",
        "updated_date": "2026-02-01T11:34:37+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Shuanghao Bai",
            "Jing Lyu",
            "Wanqi Zhou",
            "Zhe Li",
            "Dakai Wang",
            "Lei Xing",
            "Xiaoguang Zhao",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Cheng Chi",
            "Badong Chen",
            "Shanghang Zhang"
        ]
    },
    {
        "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs",
        "summary": "Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\\% success rates to as low as 2\\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.",
        "url": "http://arxiv.org/abs/2602.01158v1",
        "published_date": "2026-02-01T11:09:08+00:00",
        "updated_date": "2026-02-01T11:09:08+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Daniel Yezid Guarnizo Orjuela",
            "Leonardo Scappatura",
            "Veronica Di Gennaro",
            "Riccardo Andrea Izzo",
            "Gianluca Bardaro",
            "Matteo Matteucci"
        ]
    },
    {
        "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
        "summary": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.",
        "url": "http://arxiv.org/abs/2602.01156v1",
        "published_date": "2026-02-01T11:08:09+00:00",
        "updated_date": "2026-02-01T11:08:09+00:00",
        "categories": [
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Shunpeng Yang",
            "Ben Liu",
            "Hua Chen"
        ]
    },
    {
        "title": "UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors",
        "summary": "Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.",
        "url": "http://arxiv.org/abs/2602.01153v1",
        "published_date": "2026-02-01T11:03:01+00:00",
        "updated_date": "2026-02-01T11:03:01+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zhuo Chen",
            "Fei Ni",
            "Kaiyao Luo",
            "Zhiyuan Wu",
            "Xuyang Zhang",
            "Emmanouil Spyrakos-Papastavridis",
            "Lorenzo Jamone",
            "Nathan F. Lepora",
            "Jiankang Deng",
            "Shan Luo"
        ]
    },
    {
        "title": "KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV",
        "summary": "Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \\href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\\textcolor{red}{link}}",
        "url": "http://arxiv.org/abs/2602.01115v1",
        "published_date": "2026-02-01T09:27:56+00:00",
        "updated_date": "2026-02-01T09:27:56+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhihao Chen",
            "Yiyuan Ge",
            "Ziyang Wang"
        ]
    },
    {
        "title": "StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating",
        "summary": "Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a \"Lock-and-Gated\" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.",
        "url": "http://arxiv.org/abs/2602.01100v1",
        "published_date": "2026-02-01T08:51:17+00:00",
        "updated_date": "2026-02-01T08:51:17+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Hang Wu",
            "Tongqing Chen",
            "Jiasen Wang",
            "Xiaotao Li",
            "Lu Fang"
        ]
    },
    {
        "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
        "summary": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE",
        "url": "http://arxiv.org/abs/2602.01092v1",
        "published_date": "2026-02-01T08:16:48+00:00",
        "updated_date": "2026-02-01T08:16:48+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Peng Zhou",
            "Zhongxuan Li",
            "Jinsong Wu",
            "Jiaming Qi",
            "Jun Hu",
            "David Navarro-Alarcon",
            "Jia Pan",
            "Lihua Xie",
            "Shiyao Zhang",
            "Zeqing Zhang"
        ]
    },
    {
        "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
        "summary": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.",
        "url": "http://arxiv.org/abs/2602.01085v1",
        "published_date": "2026-02-01T08:02:30+00:00",
        "updated_date": "2026-02-01T08:02:30+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Qi Jing Chen",
            "Shilin Shan",
            "Timothy Bretl",
            "Quang-Cuong Pham"
        ]
    },
    {
        "title": "A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation",
        "summary": "Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.",
        "url": "http://arxiv.org/abs/2602.01067v1",
        "published_date": "2026-02-01T07:22:55+00:00",
        "updated_date": "2026-02-01T07:22:55+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Fanqi Lin",
            "Kushal Arora",
            "Jean Mercat",
            "Haruki Nishimura",
            "Paarth Shah",
            "Chen Xu",
            "Mengchao Zhang",
            "Mark Zolotas",
            "Maya Angeles",
            "Owen Pfannenstiehl",
            "Andrew Beaulieu",
            "Jose Barreiros"
        ]
    },
    {
        "title": "StepNav: Structured Trajectory Priors for Efficient and Multimodal Visual Navigation",
        "summary": "Visual navigation is fundamental to autonomous systems, yet generating reliable trajectories in cluttered and uncertain environments remains a core challenge. Recent generative models promise end-to-end synthesis, but their reliance on unstructured noise priors often yields unsafe, inefficient, or unimodal plans that cannot meet real-time requirements. We propose StepNav, a novel framework that bridges this gap by introducing structured, multimodal trajectory priors derived from variational principles. StepNav first learns a geometry-aware success probability field to identify all feasible navigation corridors. These corridors are then used to construct an explicit, multi-modal mixture prior that initializes a conditional flow-matching process. This refinement is formulated as an optimal control problem with explicit smoothness and safety regularization. By replacing unstructured noise with physically-grounded candidates, StepNav generates safer and more efficient plans in significantly fewer steps. Experiments in both simulation and real-world benchmarks demonstrate consistent improvements in robustness, efficiency, and safety over state-of-the-art generative planners, advancing reliable trajectory generation for practical autonomous navigation. The code has been released at https://github.com/LuoXubo/StepNav.",
        "url": "http://arxiv.org/abs/2602.02590v1",
        "published_date": "2026-02-01T06:45:42+00:00",
        "updated_date": "2026-02-01T06:45:42+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Xubo Luo",
            "Aodi Wu",
            "Haodong Han",
            "Xue Wan",
            "Wei Zhang",
            "Leizheng Shu",
            "Ruisuo Wang"
        ]
    },
    {
        "title": "LLM-Based Behavior Tree Generation for Construction Machinery",
        "summary": "Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.",
        "url": "http://arxiv.org/abs/2602.01041v1",
        "published_date": "2026-02-01T06:03:16+00:00",
        "updated_date": "2026-02-01T06:03:16+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Akinosuke Tsutsumi",
            "Tomoya Itsuka",
            "Yuichiro Kasahara",
            "Tomoya Kouno",
            "Kota Akinari",
            "Genki Yamauchi",
            "Daisuke Endo",
            "Taro Abe",
            "Takeshi Hashimoto",
            "Keiji Nagatani",
            "Ryo Kurazume"
        ]
    },
    {
        "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
        "summary": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.",
        "url": "http://arxiv.org/abs/2602.01040v1",
        "published_date": "2026-02-01T06:01:15+00:00",
        "updated_date": "2026-02-01T06:01:15+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yuhang Zhang",
            "Chao Yan",
            "Jiaxi Yu",
            "Jiaping Xiao",
            "Mir Feroskhan"
        ]
    },
    {
        "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
        "summary": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.",
        "url": "http://arxiv.org/abs/2602.01018v1",
        "published_date": "2026-02-01T05:03:58+00:00",
        "updated_date": "2026-02-01T05:03:58+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Chongyu Zhu",
            "Mithun Vanniasinghe",
            "Jiayu Chen",
            "Chi-Guhn Lee"
        ]
    },
    {
        "title": "HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving",
        "summary": "End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.",
        "url": "http://arxiv.org/abs/2602.00993v1",
        "published_date": "2026-02-01T03:15:08+00:00",
        "updated_date": "2026-02-01T03:15:08+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Weizhe Tang",
            "Junwei You",
            "Jiaxi Liu",
            "Zhaoyi Wang",
            "Rui Gan",
            "Zilin Huang",
            "Feng Wei",
            "Bin Ran"
        ]
    },
    {
        "title": "Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds",
        "summary": "In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.",
        "url": "http://arxiv.org/abs/2602.00992v1",
        "published_date": "2026-02-01T03:14:46+00:00",
        "updated_date": "2026-02-01T03:14:46+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Phone Thiha Kyaw",
            "Jonathan Kelly"
        ]
    },
    {
        "title": "Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025",
        "summary": "Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.",
        "url": "http://arxiv.org/abs/2602.00982v1",
        "published_date": "2026-02-01T02:44:52+00:00",
        "updated_date": "2026-02-01T02:44:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.NE",
            "cs.RO"
        ],
        "authors": [
            "Phu-Hoa Pham",
            "Chi-Nguyen Tran",
            "Dao Sy Duy Minh",
            "Nguyen Lam Phu Quy",
            "Huynh Trung Kiet"
        ]
    },
    {
        "title": "Meanshift Shape Formation Control Using Discrete Mass Distribution",
        "summary": "The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.",
        "url": "http://arxiv.org/abs/2602.00980v1",
        "published_date": "2026-02-01T02:44:00+00:00",
        "updated_date": "2026-02-01T02:44:00+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yichen Cai",
            "Yuan Gao",
            "Pengpeng Li",
            "Wei Wang",
            "Guibin Sun",
            "Jinhu Lü"
        ]
    },
    {
        "title": "CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining",
        "summary": "Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.",
        "url": "http://arxiv.org/abs/2602.00937v1",
        "published_date": "2026-01-31T23:32:54+00:00",
        "updated_date": "2026-01-31T23:32:54+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "I-Chun Arthur Liu",
            "Krzysztof Choromanski",
            "Sandy Huang",
            "Connor Schenck"
        ]
    },
    {
        "title": "Minimal Footprint Grasping Inspired by Ants",
        "summary": "Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.",
        "url": "http://arxiv.org/abs/2602.00935v1",
        "published_date": "2026-01-31T23:24:54+00:00",
        "updated_date": "2026-01-31T23:24:54+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Mohamed Sorour",
            "Barbara Webb"
        ]
    },
    {
        "title": "SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation",
        "summary": "The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\\%$ in simulated cluttered environments and $72.0\\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.",
        "url": "http://arxiv.org/abs/2602.00923v1",
        "published_date": "2026-01-31T22:37:27+00:00",
        "updated_date": "2026-01-31T22:37:27+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jincheng Wang",
            "Lingfan Bao",
            "Tong Yang",
            "Diego Martinez Plasencia",
            "Jianhao Jiao",
            "Dimitrios Kanoulas"
        ]
    },
    {
        "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
        "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
        "url": "http://arxiv.org/abs/2602.00919v1",
        "published_date": "2026-01-31T22:13:23+00:00",
        "updated_date": "2026-01-31T22:13:23+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "I. Apanasevich",
            "M. Artemyev",
            "R. Babakyan",
            "P. Fedotova",
            "D. Grankin",
            "E. Kupryashin",
            "A. Misailidi",
            "D. Nerus",
            "A. Nutalapati",
            "G. Sidorov",
            "I. Efremov",
            "M. Gerasyov",
            "D. Pikurov",
            "Y. Senchenko",
            "S. Davidenko",
            "D. Kulikov",
            "M. Sultankin",
            "K. Askarbek",
            "O. Shamanin",
            "D. Statovoy",
            "E. Zalyaev",
            "I. Zorin",
            "A. Letkin",
            "E. Rusakov",
            "A. Silchenko",
            "V. Vorobyov",
            "S. Sobolnikov",
            "A. Postnikov"
        ]
    },
    {
        "title": "UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation",
        "summary": "Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \\textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.",
        "url": "http://arxiv.org/abs/2602.00915v1",
        "published_date": "2026-01-31T21:56:16+00:00",
        "updated_date": "2026-01-31T21:56:16+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zhiyuan Wu",
            "Xiangyu Zhang",
            "Zhuo Chen",
            "Jiankang Deng",
            "Rolandos Alexandros Potamias",
            "Shan Luo"
        ]
    },
    {
        "title": "RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback",
        "summary": "Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.",
        "url": "http://arxiv.org/abs/2602.00886v1",
        "published_date": "2026-01-31T20:17:15+00:00",
        "updated_date": "2026-01-31T20:17:15+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Amitesh Vatsa",
            "Zhixian Xie",
            "Wanxin Jin"
        ]
    },
    {
        "title": "Learning When to Jump for Off-road Navigation",
        "summary": "Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.",
        "url": "http://arxiv.org/abs/2602.00877v1",
        "published_date": "2026-01-31T19:41:09+00:00",
        "updated_date": "2026-01-31T19:41:09+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zhipeng Zhao",
            "Taimeng Fu",
            "Shaoshu Su",
            "Qiwei Du",
            "Ehsan Tarkesh Esfahani",
            "Karthik Dantu",
            "Souma Chowdhury",
            "Chen Wang"
        ]
    },
    {
        "title": "Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects",
        "summary": "Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.",
        "url": "http://arxiv.org/abs/2602.00868v1",
        "published_date": "2026-01-31T19:07:10+00:00",
        "updated_date": "2026-01-31T19:07:10+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Nikhil Uday Shinde",
            "Dylan Hirsch",
            "Michael C. Yip",
            "Sylvia Herbert"
        ]
    }
]