[
    {
        "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
        "summary": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
        "url": "http://arxiv.org/abs/2602.02402v1",
        "published_date": "2026-02-02T17:59:31+00:00",
        "updated_date": "2026-02-02T17:59:31+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "physics.app-ph"
        ],
        "authors": [
            "Mu Huang",
            "Hui Wang",
            "Kerui Ren",
            "Linning Xu",
            "Yunsong Zhou",
            "Mulin Yu",
            "Bo Dai",
            "Jiangmiao Pang"
        ]
    },
    {
        "title": "PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning",
        "summary": "Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.",
        "url": "http://arxiv.org/abs/2602.02396v1",
        "published_date": "2026-02-02T17:57:37+00:00",
        "updated_date": "2026-02-02T17:57:37+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Amisha Bhaskar",
            "Pratap Tokekar",
            "Stefano Di Cairano",
            "Alexander Schperberg"
        ]
    },
    {
        "title": "Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures",
        "summary": "Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.",
        "url": "http://arxiv.org/abs/2602.02389v1",
        "published_date": "2026-02-02T17:51:32+00:00",
        "updated_date": "2026-02-02T17:51:32+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Marina Ruediger",
            "Ashis G. Banerjee"
        ]
    },
    {
        "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
        "summary": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.",
        "url": "http://arxiv.org/abs/2602.02331v1",
        "published_date": "2026-02-02T16:55:10+00:00",
        "updated_date": "2026-02-02T16:55:10+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Shaoting Zhu",
            "Baijun Ye",
            "Jiaxuan Wang",
            "Jiakang Chen",
            "Ziwen Zhuang",
            "Linzhan Mou",
            "Runhan Huang",
            "Hang Zhao"
        ]
    },
    {
        "title": "Before Autonomy Takes Control: Software Testing in Robotics",
        "summary": "Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.",
        "url": "http://arxiv.org/abs/2602.02293v1",
        "published_date": "2026-02-02T16:30:23+00:00",
        "updated_date": "2026-02-02T16:30:23+00:00",
        "categories": [
            "cs.SE",
            "cs.RO"
        ],
        "authors": [
            "Nils Chur",
            "Thiago Santos de Moura",
            "Argentina Ortega",
            "Sven Peldszus",
            "Thorsten Berger",
            "Nico Hochgeschwender",
            "Yannic Noller"
        ]
    },
    {
        "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems",
        "summary": "We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.",
        "url": "http://arxiv.org/abs/2602.02269v1",
        "published_date": "2026-02-02T16:11:12+00:00",
        "updated_date": "2026-02-02T16:11:12+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.SE",
            "eess.SY"
        ],
        "authors": [
            "Jon Škerlj",
            "Seongjin Bien",
            "Abdeldjallil Naceri",
            "Sami Haddadin"
        ]
    },
    {
        "title": "Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL",
        "summary": "Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.",
        "url": "http://arxiv.org/abs/2602.02236v2",
        "published_date": "2026-02-02T15:41:53+00:00",
        "updated_date": "2026-02-03T09:41:00+00:00",
        "categories": [
            "cs.RO",
            "cs.LG",
            "cs.NE",
            "eess.SY"
        ],
        "authors": [
            "Julian Lemmel",
            "Felix Resch",
            "Mónika Farsang",
            "Ramin Hasani",
            "Daniela Rus",
            "Radu Grosu"
        ]
    },
    {
        "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation",
        "summary": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap",
        "url": "http://arxiv.org/abs/2602.02220v1",
        "published_date": "2026-02-02T15:26:19+00:00",
        "updated_date": "2026-02-02T15:26:19+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Bo Miao",
            "Weijia Liu",
            "Jun Luo",
            "Lachlan Shinnick",
            "Jian Liu",
            "Thomas Hamilton-Smith",
            "Yuhe Yang",
            "Zijie Wu",
            "Vanja Videnovic",
            "Feras Dayoub",
            "Anton van den Hengel"
        ]
    },
    {
        "title": "Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls",
        "summary": "Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.",
        "url": "http://arxiv.org/abs/2602.02181v1",
        "published_date": "2026-02-02T14:49:35+00:00",
        "updated_date": "2026-02-02T14:49:35+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Elad Siman Tov",
            "Nili E. Krausz"
        ]
    },
    {
        "title": "Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding",
        "summary": "Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.",
        "url": "http://arxiv.org/abs/2602.02167v1",
        "published_date": "2026-02-02T14:44:27+00:00",
        "updated_date": "2026-02-02T14:44:27+00:00",
        "categories": [
            "eess.SP",
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Soheil Behnam Roudsari",
            "Alexandre S. Brandão",
            "Felipe N. Martins"
        ]
    },
    {
        "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
        "summary": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.",
        "url": "http://arxiv.org/abs/2602.02142v1",
        "published_date": "2026-02-02T14:19:46+00:00",
        "updated_date": "2026-02-02T14:19:46+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ruiteng Zhao",
            "Wenshuo Wang",
            "Yicheng Ma",
            "Xiaocong Li",
            "Francis E. H. Tay",
            "Marcelo H. Ang",
            "Haiyue Zhu"
        ]
    },
    {
        "title": "Frictional Contact Solving for Material Point Method",
        "summary": "Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.",
        "url": "http://arxiv.org/abs/2602.02038v1",
        "published_date": "2026-02-02T12:34:32+00:00",
        "updated_date": "2026-02-02T12:34:32+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Etienne Ménager",
            "Justin Carpentier"
        ]
    },
    {
        "title": "Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization",
        "summary": "Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.",
        "url": "http://arxiv.org/abs/2602.02035v1",
        "published_date": "2026-02-02T12:32:28+00:00",
        "updated_date": "2026-02-02T12:32:28+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Ahmad Farooq",
            "Kamran Iqbal"
        ]
    },
    {
        "title": "Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp",
        "summary": "We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.",
        "url": "http://arxiv.org/abs/2602.02026v1",
        "published_date": "2026-02-02T12:21:27+00:00",
        "updated_date": "2026-02-02T12:21:27+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zhenwei Niu",
            "Xiaoyi Chen",
            "Jiayu Hu",
            "Zhaoyang Liu",
            "Xiaozu Ju"
        ]
    },
    {
        "title": "Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements",
        "summary": "Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.\n  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.",
        "url": "http://arxiv.org/abs/2602.02006v1",
        "published_date": "2026-02-02T12:04:34+00:00",
        "updated_date": "2026-02-02T12:04:34+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Thomas Jantos",
            "Giulio Delama",
            "Stephan Weiss",
            "Jan Steinbrener"
        ]
    },
    {
        "title": "A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications",
        "summary": "Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.",
        "url": "http://arxiv.org/abs/2602.01948v1",
        "published_date": "2026-02-02T10:58:53+00:00",
        "updated_date": "2026-02-02T10:58:53+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Patrick Frank",
            "Christian Friedrich"
        ]
    },
    {
        "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
        "summary": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.",
        "url": "http://arxiv.org/abs/2602.01939v1",
        "published_date": "2026-02-02T10:43:46+00:00",
        "updated_date": "2026-02-02T10:43:46+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Yuxin He",
            "Ruihao Zhang",
            "Tianao Shen",
            "Cheng Liu",
            "Qiang Nie"
        ]
    },
    {
        "title": "LIEREx: Language-Image Embeddings for Robotic Exploration",
        "summary": "Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.",
        "url": "http://arxiv.org/abs/2602.01930v1",
        "published_date": "2026-02-02T10:30:50+00:00",
        "updated_date": "2026-02-02T10:30:50+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Felix Igelbrink",
            "Lennart Niecksch",
            "Marian Renz",
            "Martin Günther",
            "Martin Atzmueller"
        ]
    },
    {
        "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
        "summary": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/",
        "url": "http://arxiv.org/abs/2602.01916v1",
        "published_date": "2026-02-02T10:20:11+00:00",
        "updated_date": "2026-02-02T10:20:11+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Keyu Chen",
            "Wenchao Sun",
            "Hao Cheng",
            "Zheng Fu",
            "Sifa Zheng"
        ]
    },
    {
        "title": "Multi-Task Learning for Robot Perception with Imbalanced Data",
        "summary": "Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.",
        "url": "http://arxiv.org/abs/2602.01899v1",
        "published_date": "2026-02-02T10:05:59+00:00",
        "updated_date": "2026-02-02T10:05:59+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ozgur Erkent"
        ]
    },
    {
        "title": "Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study",
        "summary": "This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.",
        "url": "http://arxiv.org/abs/2602.01892v1",
        "published_date": "2026-02-02T10:03:37+00:00",
        "updated_date": "2026-02-02T10:03:37+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Alexandre Lombard",
            "Florent Perronnet",
            "Nicolas Gaud",
            "Abdeljalil Abbas-Turki"
        ]
    },
    {
        "title": "Multimodal Large Language Models for Real-Time Situated Reasoning",
        "summary": "In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.",
        "url": "http://arxiv.org/abs/2602.01880v1",
        "published_date": "2026-02-02T09:52:11+00:00",
        "updated_date": "2026-02-02T09:52:11+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Giulio Antonio Abbo",
            "Senne Lenaerts",
            "Tony Belpaeme"
        ]
    },
    {
        "title": "BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models",
        "summary": "Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.",
        "url": "http://arxiv.org/abs/2602.01870v1",
        "published_date": "2026-02-02T09:43:17+00:00",
        "updated_date": "2026-02-02T09:43:17+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Riccardo Andrea Izzo",
            "Gianluca Bardaro",
            "Matteo Matteucci"
        ]
    },
    {
        "title": "Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach",
        "summary": "Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.",
        "url": "http://arxiv.org/abs/2602.01860v1",
        "published_date": "2026-02-02T09:32:19+00:00",
        "updated_date": "2026-02-02T09:32:19+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Filip Novák",
            "Matěj Petrlík",
            "Matej Novosad",
            "Parakh M. Gupta",
            "Robert Pěnička",
            "Martin Saska"
        ]
    },
    {
        "title": "Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models",
        "summary": "Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.",
        "url": "http://arxiv.org/abs/2602.01834v1",
        "published_date": "2026-02-02T09:06:43+00:00",
        "updated_date": "2026-02-02T09:06:43+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Siqi Wen",
            "Shu Yang",
            "Shaopeng Fu",
            "Jingfeng Zhang",
            "Lijie Hu",
            "Di Wang"
        ]
    },
    {
        "title": "From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models",
        "summary": "While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.",
        "url": "http://arxiv.org/abs/2602.01811v1",
        "published_date": "2026-02-02T08:44:40+00:00",
        "updated_date": "2026-02-02T08:44:40+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Wentao Zhang",
            "Aolan Sun",
            "Wentao Mo",
            "Xiaoyang Qu",
            "Yuxin Zheng",
            "Jianzong Wang"
        ]
    },
    {
        "title": "RFS: Reinforcement learning with Residual flow steering for dexterous manipulation",
        "summary": "Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors. We propose Residual Flow Steering(RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy. We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning in both simulation and real-world settings when adapting pretrained base policies. Project website:https://weirdlabuw.github.io/rfs.",
        "url": "http://arxiv.org/abs/2602.01789v2",
        "published_date": "2026-02-02T08:11:57+00:00",
        "updated_date": "2026-02-03T05:00:30+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Entong Su",
            "Tyler Westenbroek",
            "Anusha Nagabandi",
            "Abhishek Gupta"
        ]
    },
    {
        "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models",
        "summary": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLab-SYSU/DDP-WM.",
        "url": "http://arxiv.org/abs/2602.01780v2",
        "published_date": "2026-02-02T08:04:25+00:00",
        "updated_date": "2026-02-03T02:48:57+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Shicheng Yin",
            "Kaixuan Yin",
            "Weixing Chen",
            "Yang Liu",
            "Guanbin Li",
            "Liang Lin"
        ]
    },
    {
        "title": "Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion",
        "summary": "Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.",
        "url": "http://arxiv.org/abs/2602.01731v1",
        "published_date": "2026-02-02T07:12:27+00:00",
        "updated_date": "2026-02-02T07:12:27+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jiwoo Hwang",
            "Taegeun Yang",
            "Jeil Jeong",
            "Minsung Yoon",
            "Sung-Eui Yoon"
        ]
    },
    {
        "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
        "summary": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.",
        "url": "http://arxiv.org/abs/2602.01700v1",
        "published_date": "2026-02-02T06:15:33+00:00",
        "updated_date": "2026-02-02T06:15:33+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Ruoyu Wang",
            "Xuchen Liu",
            "Zongzhou Wu",
            "Zixuan Guo",
            "Wendi Ding",
            "Ben M. Chen"
        ]
    },
    {
        "title": "GSR: Learning Structured Reasoning for Embodied Manipulation",
        "summary": "Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.",
        "url": "http://arxiv.org/abs/2602.01693v1",
        "published_date": "2026-02-02T06:07:42+00:00",
        "updated_date": "2026-02-02T06:07:42+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Kewei Hu",
            "Michael Zhang",
            "Wei Ying",
            "Tianhao Liu",
            "Guoqiang Hao",
            "Zimeng Li",
            "Wanchan Yu",
            "Jiajian Jing",
            "Fangwen Chen",
            "Hanwen Kang"
        ]
    },
    {
        "title": "Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications",
        "summary": "The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.",
        "url": "http://arxiv.org/abs/2602.01679v1",
        "published_date": "2026-02-02T05:46:31+00:00",
        "updated_date": "2026-02-02T05:46:31+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Raghavasimhan Sankaranarayanan",
            "Paul Stuart",
            "Nicholas Ahn",
            "Arno Sungarian",
            "Yash Chitalia"
        ]
    },
    {
        "title": "Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss",
        "summary": "Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.",
        "url": "http://arxiv.org/abs/2602.01673v1",
        "published_date": "2026-02-02T05:41:42+00:00",
        "updated_date": "2026-02-02T05:41:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Enguang Fan"
        ]
    },
    {
        "title": "AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act",
        "summary": "Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.",
        "url": "http://arxiv.org/abs/2602.01662v1",
        "published_date": "2026-02-02T05:30:14+00:00",
        "updated_date": "2026-02-02T05:30:14+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Pengyuan Guo",
            "Zhonghao Mai",
            "Zhengtong Xu",
            "Kaidi Zhang",
            "Heng Zhang",
            "Zichen Miao",
            "Arash Ajoudani",
            "Zachary Kingston",
            "Qiang Qiu",
            "Yu She"
        ]
    },
    {
        "title": "From Perception to Action: Spatial AI Agents and World Models",
        "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.",
        "url": "http://arxiv.org/abs/2602.01644v1",
        "published_date": "2026-02-02T05:00:55+00:00",
        "updated_date": "2026-02-02T05:00:55+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.MA",
            "cs.RO"
        ],
        "authors": [
            "Gloria Felicia",
            "Nolan Bryant",
            "Handi Putra",
            "Ayaan Gazali",
            "Eliel Lobo",
            "Esteban Rojas"
        ]
    },
    {
        "title": "A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation",
        "summary": "Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.",
        "url": "http://arxiv.org/abs/2602.01632v1",
        "published_date": "2026-02-02T04:44:55+00:00",
        "updated_date": "2026-02-02T04:44:55+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Chuizheng Kong",
            "Yunho Cho",
            "Wonsuhk Jung",
            "Idris Wibowo",
            "Parth Shinde",
            "Sundhar Vinodh-Sangeetha",
            "Long Kiu Chung",
            "Zhenyang Chen",
            "Andrew Mattei",
            "Advaith Nidumukkala",
            "Alexander Elias",
            "Danfei Xu",
            "Taylor Higgins",
            "Shreyas Kousik"
        ]
    },
    {
        "title": "AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments",
        "summary": "Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \\textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.",
        "url": "http://arxiv.org/abs/2602.01629v1",
        "published_date": "2026-02-02T04:41:35+00:00",
        "updated_date": "2026-02-02T04:41:35+00:00",
        "categories": [
            "cs.LG",
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Renukanandan Tumu",
            "Aditya Singh",
            "Rahul Mangharam"
        ]
    },
    {
        "title": "Efficiently Solving Mixed-Hierarchy Games with Quasi-Policy Approximations",
        "summary": "Multi-robot coordination often exhibits hierarchical structure, with some robots' decisions depending on the planned behaviors of others. While game theory provides a principled framework for such interactions, existing solvers struggle to handle mixed information structures that combine simultaneous (Nash) and hierarchical (Stackelberg) decision-making. We study N-robot forest-structured mixed-hierarchy games, in which each robot acts as a Stackelberg leader over its subtree while robots in different branches interact via Nash equilibria. We derive the Karush-Kuhn-Tucker (KKT) first-order optimality conditions for this class of games and show that they involve increasingly high-order derivatives of robots' best-response policies as the hierarchy depth grows, rendering a direct solution intractable. To overcome this challenge, we introduce a quasi-policy approximation that removes higher-order policy derivatives and develop an inexact Newton method for efficiently solving the resulting approximated KKT systems. We prove local exponential convergence of the proposed algorithm for games with non-quadratic objectives and nonlinear constraints. The approach is implemented in a highly optimized Julia library (MixedHierarchyGames.jl) and evaluated in simulated experiments, demonstrating real-time convergence for complex mixed-hierarchy information structures.",
        "url": "http://arxiv.org/abs/2602.01568v1",
        "published_date": "2026-02-02T03:03:29+00:00",
        "updated_date": "2026-02-02T03:03:29+00:00",
        "categories": [
            "cs.GT",
            "cs.RO"
        ],
        "authors": [
            "Hamzah Khan",
            "Dong Ho Lee",
            "Jingqi Li",
            "Tianyu Qiu",
            "Christian Ellis",
            "Jesse Milzman",
            "Wesley Suttle",
            "David Fridovich-Keil"
        ]
    },
    {
        "title": "UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning",
        "summary": "Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.",
        "url": "http://arxiv.org/abs/2602.01536v1",
        "published_date": "2026-02-02T02:10:51+00:00",
        "updated_date": "2026-02-02T02:10:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Shuai Liu",
            "Siheng Ren",
            "Xiaoyao Zhu",
            "Quanmin Liang",
            "Zefeng Li",
            "Qiang Li",
            "Xin Hu",
            "Kai Huang"
        ]
    },
    {
        "title": "Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations",
        "summary": "While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.",
        "url": "http://arxiv.org/abs/2602.01535v1",
        "published_date": "2026-02-02T02:10:34+00:00",
        "updated_date": "2026-02-02T02:10:34+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Huzaifa Mustafa Unjhawala",
            "Khizar Shaikh",
            "Luning Bakke",
            "Radu Serban",
            "Dan Negrut"
        ]
    },
    {
        "title": "RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots",
        "summary": "Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.",
        "url": "http://arxiv.org/abs/2602.01515v1",
        "published_date": "2026-02-02T01:04:55+00:00",
        "updated_date": "2026-02-02T01:04:55+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Humphrey Munn",
            "Brendan Tidd",
            "Peter Bohm",
            "Marcus Gallagher",
            "David Howard"
        ]
    },
    {
        "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching",
        "summary": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.",
        "url": "http://arxiv.org/abs/2602.01501v2",
        "published_date": "2026-02-02T00:32:07+00:00",
        "updated_date": "2026-02-03T08:23:36+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Minwoo Jung",
            "Nived Chebrolu",
            "Lucas Carvalho de Lima",
            "Haedam Oh",
            "Maurice Fallon",
            "Ayoung Kim"
        ]
    },
    {
        "title": "Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression",
        "summary": "This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable \"ring mechanism\", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.",
        "url": "http://arxiv.org/abs/2602.01448v1",
        "published_date": "2026-02-01T21:33:47+00:00",
        "updated_date": "2026-02-01T21:33:47+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Harshith Jella",
            "Pejman Kheradmand",
            "Joseph Klein",
            "Behnam Moradkhani",
            "Yash Chitalia"
        ]
    },
    {
        "title": "Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors",
        "summary": "This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.",
        "url": "http://arxiv.org/abs/2602.01429v1",
        "published_date": "2026-02-01T20:32:31+00:00",
        "updated_date": "2026-02-01T20:32:31+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Gonzalo Olguin",
            "Javier Ruiz-del-Solar"
        ]
    },
    {
        "title": "Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation",
        "summary": "Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.",
        "url": "http://arxiv.org/abs/2602.01389v1",
        "published_date": "2026-02-01T18:49:03+00:00",
        "updated_date": "2026-02-01T18:49:03+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Michele Antonazzi",
            "Lorenzo Signorelli",
            "Matteo Luperto",
            "Nicola Basilico"
        ]
    },
    {
        "title": "TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design",
        "summary": "Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.",
        "url": "http://arxiv.org/abs/2602.01385v1",
        "published_date": "2026-02-01T18:38:22+00:00",
        "updated_date": "2026-02-01T18:38:22+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Xiangyu Li",
            "Mingwei Lai",
            "Mengke Zhang",
            "Junxiao Lin",
            "Tiancheng Lai",
            "Junping Zhi",
            "Chao Xu",
            "Fei Gao",
            "Yanjun Cao"
        ]
    }
]